cd ~ && VLLM_HINT_SERVER_URL="http://localhost:5000/hint" \
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-4B-Instruct-2507 --port 8000 2>&1 | tee /tmp/vllm.log